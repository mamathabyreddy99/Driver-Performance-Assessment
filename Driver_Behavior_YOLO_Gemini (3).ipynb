{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "--DcSHEvOlc1"
      },
      "id": "--DcSHEvOlc1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚗 Driver Behavior Analysis Using YOLOv8 and Gemini AI\n",
        "📄 Project Description:\n",
        "This project focuses on analyzing driver behavior at traffic intersections using a combination of computer vision and generative AI. The system uses a custom-trained YOLOv8 model to detect key traffic elements (such as stop signs, traffic lights, and speed limits) from dashcam video footage. The detected behaviors are then passed to Gemini 1.5 Flash, a large language model by Google, which provides natural language evaluations of driving compliance or violations.\n",
        "\n",
        "The full pipeline includes:\n",
        "\n",
        "Frame extraction from raw dashcam videos\n",
        "\n",
        "Object detection using YOLOv8\n",
        "\n",
        "Event description generation and selection of frames\n",
        "\n",
        "Behavior evaluation using Gemini (Google Generative AI)\n",
        "\n",
        "Automatic report generation for each driving event\n",
        "\n",
        "This hybrid AI system simulates the decision-making process of a human driving examiner and is useful for traffic safety research, driver feedback systems, or autonomous vehicle audits."
      ],
      "metadata": {
        "id": "PhlMrS9AOnSX"
      },
      "id": "PhlMrS9AOnSX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5IkazxYa9eU"
      },
      "source": [
        "# 🚗 Driver Behavior Analysis with YOLO and Gemini\n",
        "This notebook:\n",
        "- Uploads a driving video\n",
        "- Extracts frames every 2 seconds\n",
        "- Runs detection using your YOLO model\n",
        "- Sends YOLO-detected frames to Gemini AI for behavior analysis\n",
        "- Saves responses into a report"
      ],
      "id": "k5IkazxYa9eU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🟩 STEP 1: Install Required Libraries\n"
      ],
      "metadata": {
        "id": "QI2oIZDMM3h3"
      },
      "id": "QI2oIZDMM3h3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1JvYg7tda9eV",
        "outputId": "3f67ffd9-a124-405f-b785-216c79d597be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.134-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.169.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.134-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.134 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "# ✅ Install dependencies\n",
        "!pip install ultralytics google-generativeai opencv-python pillow"
      ],
      "id": "1JvYg7tda9eV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66a_uvwa9eV",
        "outputId": "a40ca5f0-a5d9-47f5-a374-8b807f198981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# ✅ Import libraries\n",
        "from ultralytics import YOLO\n",
        "import cv2, os, base64\n",
        "from PIL import Image\n",
        "import google.generativeai as genai"
      ],
      "id": "c66a_uvwa9eV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🟩 STEP 2: Set Up Environment and Mount Google Drive"
      ],
      "metadata": {
        "id": "9FdxVxu9NmS4"
      },
      "id": "9FdxVxu9NmS4"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SssXAISUNnqQ",
        "outputId": "46de748a-0db0-4476-ac64-52a304d22b46"
      },
      "id": "SssXAISUNnqQ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🟩 STEP 3: Extract Frames from Dashcam Videos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3o28kKSQIu14"
      },
      "id": "3o28kKSQIu14"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# === INPUT VIDEO PATH ===\n",
        "video_path = '/content/m2-res_1080p (3).mp4'\n",
        "output_folder = 'frames_every_8_sec'\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)  # Get frames per second\n",
        "frame_interval = int(fps * 1.5 )   # 2 seconds worth of frames\n",
        "\n",
        "frame_count = 0\n",
        "saved_count = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count % frame_interval == 0:\n",
        "        filename = os.path.join(output_folder, f\"frame_{saved_count:04d}.jpg\")\n",
        "        cv2.imwrite(filename, frame)\n",
        "        saved_count += 1\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "print(f\"Saved {saved_count} frames.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osJCmyQfzal4",
        "outputId": "36889a86-f821-4f61-d008-d937cb0a9b2e"
      },
      "id": "osJCmyQfzal4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 13 frames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split 4 driving events from the renamed video\n",
        "!ffmpeg -i \"/content/driving_test.mp4\" -ss 00:04:20 -to 00:04:35 -c copy event1.mp4\n",
        "!ffmpeg -i \"/content/driving_test.mp4\" -ss 00:05:40 -to 00:05:50 -c copy event2.mp4\n",
        "# !ffmpeg -i \"/content/driving_test.mp4\" -ss 00:07:32 -to 00:07:42 -c copy event3.mp4\n",
        "# !ffmpeg -i \"/content/driving_test.mp4\" -ss 00:08:30 -to 00:08:40 -c copy event4.mp4\n"
      ],
      "metadata": {
        "id": "j5gX5qB5uuqP"
      },
      "id": "j5gX5qB5uuqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "🟩 STEP 4: Run Detection Using Your YOLO Model **bold text**\n",
        "Objective: Use your trained YOLOv8 model (best.pt) to detect driver behavior events from extracted dashcam frames. The model performs inference on images and saves annotated results to output folders.\n",
        "\n"
      ],
      "metadata": {
        "id": "oyR-R8TQKsVR"
      },
      "id": "oyR-R8TQKsVR"
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Load both models\n",
        "custom_model = YOLO(\"/content/drive/MyDrive/YOLOv8_models/best.pt\")  # correct path to .pt file\n",
        "coco_model = YOLO(\"yolov8n.pt\")  # pre-trained COCO model\n",
        "\n",
        "def detect_and_save(model, input_folder, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for img_file in sorted(os.listdir(input_folder)):\n",
        "        if img_file.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(input_folder, img_file)\n",
        "            results = model(img_path)\n",
        "            results[0].save(filename=os.path.join(output_folder, img_file))\n",
        "\n",
        "# Run detection for both models\n",
        "detect_and_save(custom_model, \"eventc11_combined\", \"eventc1_combined\")\n",
        "detect_and_save(coco_model, \"eventc11_combined\", \"eventc1coco\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iOM1MSU0NabK",
        "outputId": "68b75b34-1142-44fa-e9a5-8fc4f06c8879"
      },
      "id": "iOM1MSU0NabK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0000.jpg: 384x640 (no detections), 241.7ms\n",
            "Speed: 10.0ms preprocess, 241.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0001.jpg: 384x640 (no detections), 212.4ms\n",
            "Speed: 4.4ms preprocess, 212.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0002.jpg: 384x640 3 Green Lights, 169.7ms\n",
            "Speed: 4.3ms preprocess, 169.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0003.jpg: 384x640 (no detections), 134.3ms\n",
            "Speed: 3.2ms preprocess, 134.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0004.jpg: 384x640 (no detections), 139.6ms\n",
            "Speed: 3.3ms preprocess, 139.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0005.jpg: 384x640 (no detections), 141.8ms\n",
            "Speed: 3.0ms preprocess, 141.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0006.jpg: 384x640 1 Red Light, 136.0ms\n",
            "Speed: 3.1ms preprocess, 136.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0007.jpg: 384x640 (no detections), 130.6ms\n",
            "Speed: 2.9ms preprocess, 130.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0008.jpg: 384x640 1 Red Light, 149.7ms\n",
            "Speed: 3.2ms preprocess, 149.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0009.jpg: 384x640 (no detections), 133.3ms\n",
            "Speed: 3.0ms preprocess, 133.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0010.jpg: 384x640 (no detections), 143.0ms\n",
            "Speed: 2.9ms preprocess, 143.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0011.jpg: 384x640 (no detections), 136.1ms\n",
            "Speed: 3.0ms preprocess, 136.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0012.jpg: 384x640 (no detections), 134.6ms\n",
            "Speed: 2.9ms preprocess, 134.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0000.jpg: 384x640 8 cars, 1 traffic light, 159.4ms\n",
            "Speed: 3.1ms preprocess, 159.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0001.jpg: 384x640 6 cars, 1 traffic light, 142.6ms\n",
            "Speed: 3.1ms preprocess, 142.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0002.jpg: 384x640 7 cars, 1 traffic light, 143.0ms\n",
            "Speed: 2.9ms preprocess, 143.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0003.jpg: 384x640 8 cars, 1 traffic light, 151.3ms\n",
            "Speed: 3.0ms preprocess, 151.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0004.jpg: 384x640 4 cars, 1 truck, 3 traffic lights, 143.6ms\n",
            "Speed: 2.9ms preprocess, 143.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0005.jpg: 384x640 7 cars, 2 traffic lights, 158.8ms\n",
            "Speed: 3.0ms preprocess, 158.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0006.jpg: 384x640 5 cars, 2 traffic lights, 156.9ms\n",
            "Speed: 3.0ms preprocess, 156.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0007.jpg: 384x640 2 persons, 3 cars, 1 traffic light, 150.2ms\n",
            "Speed: 3.0ms preprocess, 150.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0008.jpg: 384x640 3 cars, 3 traffic lights, 1 kite, 144.5ms\n",
            "Speed: 2.9ms preprocess, 144.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0009.jpg: 384x640 4 persons, 1 car, 1 traffic light, 147.4ms\n",
            "Speed: 3.0ms preprocess, 147.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0010.jpg: 384x640 1 car, 161.7ms\n",
            "Speed: 3.2ms preprocess, 161.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0011.jpg: 384x640 1 car, 157.4ms\n",
            "Speed: 3.1ms preprocess, 157.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/eventc11_combined/frame_0012.jpg: 384x640 4 cars, 138.7ms\n",
            "Speed: 3.1ms preprocess, 138.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: Connect to Gemini (LLM) for Behavior Summary or Interaction\n",
        "# This step sets up the Gemini 1.5 API to allow us to generate natural language content\n",
        "# Example: summarizing behavior detection results, answering driver-related questions, etc.\n"
      ],
      "metadata": {
        "id": "-i_kScJZLg5e"
      },
      "id": "-i_kScJZLg5e"
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=\"AIzaSyALgQf22wWiuanzmqZjgor_ktcgsV8NH44\")\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response = model.generate_content(\"Hi Gemini, are you online?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hSn8VkJDg3Cu",
        "outputId": "5d73e74f-739c-4b2b-b62a-9d2c8b9a440a"
      },
      "id": "hSn8VkJDg3Cu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I'm online and ready to assist you.  How can I help?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: Generate Behavior Evaluation Prompt for Gemini\n",
        "# This function creates a detailed prompt that tells Gemini to analyze driver behavior\n",
        "# using your YOLO detection results and dashcam frame sequence context.\n"
      ],
      "metadata": {
        "id": "h0JZyRaULzmp"
      },
      "id": "h0JZyRaULzmp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT 1"
      ],
      "metadata": {
        "id": "gv9wumwEL6od"
      },
      "id": "gv9wumwEL6od"
    },
    {
      "cell_type": "code",
      "source": [
        "def driving_prompt_successive(event_description, frames):\n",
        "    frame_list = \", \".join(frames)\n",
        "    return f\"\"\"\n",
        "You are a certified driving examiner evaluating a driver's behavior at an intersection, using a sequence of **successive dashcam frames** captured in time order.\n",
        "\n",
        "This event occurs at or near a **traffic-controlled intersection**, possibly with **stop signs** and/or **traffic lights**.\n",
        "\n",
        "**Event Description:** {event_description}\n",
        "**Frames Provided (in time order):** {frame_list}\n",
        "\n",
        "Your Task:\n",
        "- Analyze what kind of intersection this is\n",
        "- Identify any traffic signals, stop signs, or markings\n",
        "- Evaluate how the driver behaved across the sequence\n",
        "- Determine if rules were followed or violated\n",
        "\n",
        "Respond using this structure:\n",
        "\n",
        "1. **Scene Interpretation:**\n",
        "   - What type of intersection is this? (4-way, T-junction, etc.)\n",
        "   - Are there stop signs or traffic lights? What is their state?\n",
        "   - What do you observe around the vehicle (e.g., signs, road lines, cars, pedestrians)?\n",
        "\n",
        "2. **Driver Behavior Over Time:**\n",
        "   - What does the driver do as they approach or pass through the intersection?\n",
        "   - Do they stop, slow, proceed without stopping, or yield correctly?\n",
        "   - If the light is green, did they proceed safely?\n",
        "   - If a stop sign is visible, did they stop completely and safely?\n",
        "\n",
        "3. **Violation or Risk Assessment:**\n",
        "   - Did the driver fail to stop or yield when required?\n",
        "   - Did they proceed through a red light or ignore a stop sign?\n",
        "   - If behavior was compliant, say so clearly.\n",
        "\n",
        "4. **Visual Evidence Used:**\n",
        "   - Mention what objects support your conclusion: signs, light color, stop line, markings, car motion, etc.\n",
        "\n",
        "5. **Final Evaluation and Advice:**\n",
        "   - Verdict: Compliant / Violation / Needs Improvement\n",
        "   - Suggest what the driver should have done differently (if anything)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "xqEVXiPckokG"
      },
      "id": "xqEVXiPckokG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT 2"
      ],
      "metadata": {
        "id": "wqyCyoH8L4Pe"
      },
      "id": "wqyCyoH8L4Pe"
    },
    {
      "cell_type": "code",
      "source": [
        "# def driving_prompt_successive(event_description, frames):\n",
        "#     frame_list = \", \".join(frames)\n",
        "#     return f\"\"\" You are a certified driving examiner evaluating a driver's behavior at an intersection, using a sequence of **successive dashcam frames** captured in time order.\n",
        "\n",
        "#  This event occurs at or near a **traffic-controlled intersection**, possibly with **stop signs** and/or **traffic lights**.\n",
        "\n",
        "#  **Event Description:** {event_description}\n",
        "#  **Frames Provided (in time order):** {frame_list}\n",
        "\n",
        "\n",
        "# These frames are captured in time order and show the driver's behavior during this event.\n",
        "# Input:\n",
        "\n",
        "# Successive Frames (Time Sequence):\n",
        "# A series of successive frames captured over time, showing the driver's environment and behavior.\n",
        "\n",
        "# Detected Scene Objects:\n",
        "# A structured list of detected objects across all frames (e.g., traffic signs, vehicles, pedestrians, road infrastructure).\n",
        "\n",
        "# Driver Actions:\n",
        "# A time-sequenced description of the driver’s behavior over the successive frames (e.g., speed changes, lane changes, turns, stops).\n",
        "\n",
        "# Evaluation Score Sheet:\n",
        "# A list of rule-based penalties or scoring criteria (e.g., “Failure to yield to pedestrian = -15 points”).\n",
        "\n",
        "# Task:\n",
        "# Analyze the input using step-by-step reasoning based on the full sequence of successive frames. Think carefully at each step. Follow this structure:\n",
        "\n",
        "# Reasoning Steps:\n",
        "\n",
        "# 1. Scene Interpretation:\n",
        "#    - What are the relevant objects visible across the sequence of frames?\n",
        "#    - What potential hazards, rules, or expectations are implied by their presence over time?\n",
        "\n",
        "# 2. Action Analysis:\n",
        "#    - What did the driver do across the full sequence (e.g., approaching, stopping, yielding)?\n",
        "#    - Was the driver’s overall behavior safe and compliant with standard driving rules?\n",
        "\n",
        "# 3. Rule Matching:\n",
        "#    - Does the driver’s behavior match or violate any item from the evaluation score sheet?\n",
        "#    - Identify specific rules and penalties, if applicable.\n",
        "\n",
        "# 4. Score Calculation:\n",
        "#    - Calculate the total deduction (if any).\n",
        "#    - Mention the rule numbers or categories used.\n",
        "\n",
        "# 5. Decision and Feedback:\n",
        "#    - Provide a final evaluation (compliant / needs improvement / violation).\n",
        "#    - Suggest improvements or corrections the driver should make.\n",
        "\n",
        "# Important Rules:\n",
        "# - Analyze the driver's behavior as a sequence (not frame-by-frame individually).\n",
        "# - Base your judgment ONLY on what is clearly visible across the successive frames.\n",
        "# - Do NOT assume or guess anything outside the frames.\n",
        "# - Focus strictly on the behavior shown over time.\n",
        "\n",
        "# Example Input:\n",
        "# Frames 0–4:\n",
        "# Scene Objects Detected:\n",
        "# - Frame 0: Stop Sign visible ahead (20 meters), Car ahead slowing.\n",
        "# - Frame 1: Stop Sign closer (10 meters), Car ahead braking.\n",
        "# - Frame 2: Driver slows down.\n",
        "# - Frame 3: Driver stops fully.\n",
        "# - Frame 4: Cross traffic moving through intersection.\n",
        "\n",
        "# Driver Actions:\n",
        "# - Driver slows down and makes a full stop before stop sign.\n",
        "\n",
        "# Score Sheet:\n",
        "#   Rule 3.2: Failure to stop at stop sign = -10 pts\n",
        "\n",
        "# Expected Output:\n",
        "\n",
        "# Scene Interpretation:\n",
        "# The stop sign indicates a legal requirement to stop. Cross traffic movement confirms need to yield right-of-way.\n",
        "\n",
        "# Action Analysis:\n",
        "# The driver slowed appropriately and made a full stop before the stop sign, yielding to cross traffic.\n",
        "\n",
        "# Rule Matching:\n",
        "# - No violations observed.\n",
        "\n",
        "# Score Calculation:\n",
        "# - No deductions.\n",
        "\n",
        "# Decision and Feedback:\n",
        "# ✅ Driver behavior is compliant.\n",
        "# ✅ Recommendation: Continue practicing defensive and attentive driving at intersections.\n",
        "# \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "uwWJCUOg9Lkv"
      },
      "id": "uwWJCUOg9Lkv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟩 STEP 7: Load Selected Frames and Save Gemini's Evaluation as Report\n",
        "# - load_images(): loads the dashcam frames associated with the current event\n",
        "# - save_report(): saves Gemini’s behavior analysis to a text file for later review or documentation\n"
      ],
      "metadata": {
        "id": "lYULf88MMKJx"
      },
      "id": "lYULf88MMKJx"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_images(frame_paths):\n",
        "    return [Image.open(f) for f in frame_paths if os.path.exists(f)]\n",
        "\n",
        "def save_report(event_name, description, response):\n",
        "    with open(f\"{event_name}_report.txt\", \"w\") as f:\n",
        "        f.write(f\"Event Description: {description}\\n\\n\")\n",
        "        f.write(response)"
      ],
      "metadata": {
        "id": "bBlQsyyjoX8y"
      },
      "id": "bBlQsyyjoX8y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_event(event_name, frames, event_description):\n",
        "    prompt = driving_prompt_successive(event_description, frames)\n",
        "    images = load_images(frames)\n",
        "    response = model.generate_content([prompt] + images, stream=False)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "jLL8zPYRoXqL"
      },
      "id": "jLL8zPYRoXqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟩 STEP 8: Run Behavior Analysis for a Specific Event\n",
        "# This block sends the selected dashcam frames and event description to Gemini,\n",
        "# receives the behavior evaluation, and saves it as a report.\n"
      ],
      "metadata": {
        "id": "k3a-P-bpMVtS"
      },
      "id": "k3a-P-bpMVtS"
    },
    {
      "cell_type": "code",
      "source": [
        "event_name = \"eventc1\"\n",
        "frames = [\n",
        "    \"/content/eventc1_combined/frame_0000.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0001.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0002.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0003.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0004.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0005.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0006.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0007.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0008.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0009.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0010.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0011.jpg\",\n",
        "    \"/content/eventc1_combined/frame_0012.jpg\"\n",
        "]\n",
        "\n",
        "description = \"Driver approaching an intersection\"\n",
        "response = analyze_event(event_name, frames, description)\n",
        "save_report(event_name, description, response)\n",
        "\n",
        "print(f\"✅ Saved {event_name}_report.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2WxVtPdKoej0",
        "outputId": "b78c1e3f-7ddb-44fa-9348-280095bda110"
      },
      "id": "2WxVtPdKoej0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved eventc1_report.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/eventc1_combined /content/eventc1_combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M1-D-cJxTf2",
        "outputId": "26b2e07c-625a-4579-a09d-aeb6c6c3bb89",
        "collapsed": true
      },
      "id": "-M1-D-cJxTf2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/eventc1_combined/ (stored 0%)\n",
            "  adding: content/eventc1_combined/frame_0001.jpg (deflated 4%)\n",
            "  adding: content/eventc1_combined/frame_0012.jpg (deflated 0%)\n",
            "  adding: content/eventc1_combined/frame_0007.jpg (deflated 1%)\n",
            "  adding: content/eventc1_combined/frame_0000.jpg (deflated 4%)\n",
            "  adding: content/eventc1_combined/frame_0009.jpg (deflated 0%)\n",
            "  adding: content/eventc1_combined/frame_0008.jpg (deflated 1%)\n",
            "  adding: content/eventc1_combined/frame_0006.jpg (deflated 1%)\n",
            "  adding: content/eventc1_combined/frame_0005.jpg (deflated 1%)\n",
            "  adding: content/eventc1_combined/frame_0004.jpg (deflated 1%)\n",
            "  adding: content/eventc1_combined/frame_0010.jpg (deflated 0%)\n",
            "  adding: content/eventc1_combined/frame_0003.jpg (deflated 4%)\n",
            "  adding: content/eventc1_combined/frame_0011.jpg (deflated 0%)\n",
            "  adding: content/eventc1_combined/frame_0002.jpg (deflated 4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟩 STEP 9: Run YOLO Inference on Full Video and Save Annotated Output\n",
        "# This step loads the original video, detects driver behavior in each frame using YOLOv8,\n",
        "# and saves a new annotated video showing all bounding boxes and labels.\n"
      ],
      "metadata": {
        "id": "CPnS4RbvMjkN"
      },
      "id": "CPnS4RbvMjkN"
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# === Configuration ==='\n",
        "input_video_path = '/content/m2-res_1080p (3).mp4'\n",
        "output_video_path = 'yolo_output.mp4'    # Output path\n",
        "model_path = '/content/drive/MyDrive/YOLOv8_models/best.pt'                   # Your YOLO model (pt or onnx)\n",
        "conf_threshold = 0.3                     # Minimum confidence to draw boxes\n",
        "\n",
        "# === Load Model ===\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# === Load Video ===\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# === Video Writer ===\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# === Detection Loop ===\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO inference\n",
        "    results = model.predict(source=frame, conf=conf_threshold, save=False, verbose=False)\n",
        "    annotated = results[0].plot()  # Draw bounding boxes\n",
        "\n",
        "    out.write(annotated)  # Save frame to output video\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"✅ Saved YOLO output video to: {output_video_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEwBU57hxuj3",
        "outputId": "6cb5e96c-ea0b-45be-aa98-96b850d2b64d"
      },
      "id": "EEwBU57hxuj3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved YOLO output video to: yolo_output.mp4\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}